###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\nclient<llm> Ollama {\n  provider \"openai-generic\"\n  options {\n    base_url \"http://192.168.1.188:11434/v1/\"\n    //model \"deepseek-r1:32b\"\n    model \"gemma3:27b\"\n  }\n}\n\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    mutliplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "deduce_choice.baml": "// Defining a data model.\nclass Choice {\n  choice string\n  no_available_information bool\n}\n\n// Create a function to extract the resume from a string.\nfunction DeduceChoice(instruction: string, memory: string) -> Choice {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"Ollama\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Based on the provided options and related answers, choose one option to respond to the question '{{ instruction }}'. No explanation is needed; If there are no available options, simply set the no_available_information boolean to true in the output without any other explanation otherwise set it to false. Ensure that the information provided comes directly and accurately from the retrieved document, without any speculation.\n    \n    Information: \n    \n    {{ memory }}\n       \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest laibhav_resume {\n  functions [DeduceChoice]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "deduce_entail.baml": "// Defining a data model.\nclass Entail {\n  entail string\n  no_available_information bool\n}\n\n// Create a function to extract the resume from a string.\nfunction DeduceEntail(instruction: string, memory: string) -> Entail {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"Ollama\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Based on the provided information, first determine whether you can directly respond to the instruction '{{ instruction }}'. If you can directly answer, reply with the answer without any explanation; if you cannot answer directly but there is related information, summarize the key information related to the instruction '{{ instruction }}' and clearly explain why it is related; if there is no relevant information, simply set the no_available_information boolean in your response without explanation.\n    \n    [Information]: '{{ memory }}'\n\n    Ensure that the information provided comes directly and accurately from the retrieved document, without any speculation.\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest laibhav_resume {\n  functions [DeduceEntail]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "deduce_judge.baml": "// Defining a data model.\nclass Judge {\n  assessable bool\n  no_available_information bool\n}\n\n// Create a function to extract the resume from a string.\nfunction DeduceJudge(instruction: string, memory: string) -> Judge {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"Ollama\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Based on the provided information, first determine if the question '{{ instruction }}' can be directly assessed. If it can be directly answered, simply respond with true or false if assessable based on the provided information, no explanation needed; If there is no relevant information, simply set the boolean no_available_information in your response without explanation.\n    \n    Information: \n    \n    {{ memory }}\n        \n    Ensure that the information provided comes directly and accurately from the retrieved document, without any speculation.\n      \n    Question: '{{ instruction }}'\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest laibhav_resume {\n  functions [DeduceJudge]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "deduce_multi_choice.baml": "// Defining a data model.\nclass Multi {\n  option string\n  no_available_information bool\n}\n\n// Create a function to extract the resume from a string.\nfunction DeduceMultiChoice(instruction: string, memory: string) -> Multi {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"Ollama\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Based on the provided options and related answers, choose at least one option to respond to the question '{{ instruction }}'. No explanation is needed; If there are no available options, simply set the no_available_information boolean to false without explanation. Ensure that the information provided comes directly and accurately from the retrieved document, without any speculation.\n    \n    Information: \n    \n    {{ memory }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest laibhav_resume {\n  functions [DeduceMultiChoice]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.78.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "logic_from_plan.baml": "// Defining a data model.\nclass Plan {\n  step string\n  action string\n}\n\nclass Plans {\n    plans Plan[]\n}\n\n// Create a function to extract the resume from a string.\nfunction LogicFromPlan(question: string) -> Plans {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"Ollama\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n  {\n  \"instruction\": \"Decompose the query below into its individual logical steps in the form of groups of simple question (Step) and resulting action (Action) following the provided examples. Make sure the Step is a question and respond in JSON.\",\n  \"function_description\": \"functionName is operator name; the function format is functionName(arg_name1=arg_value1,[args_name2=arg_value2, args_name3=arg_value3]), The parameters in parentheses are the arguments. Parameters enclosed in [] are optional, while those not enclosed in [] are required.\",\n    \"function\": [\n      {\n          \"functionName\": \"get_spo\",\n          \"function_declaration\": \"get_spo(s=s_alias:entity_type[entity_name], p=p_alias:edge_type, o=o_alias:entity_type[entity_name])\",\n          \"description\": \"Find SPO information. 's' represents the subject, 'o' represents the object, and they are denoted as variable_name:entity_type[entity_name]. The entity name is an optional parameter and should be provided when there is a specific entity to query. 'p' represents the predicate, which can be a relationship or attribute, denoted as variable_name:edge_type_or_attribute_type. Each variable is assigned a unique variable name, which is used for reference in subsequent mentions. Note that 's', 'p', and 'o' should not appear repeatedly within the same expression; only one set of SPO should be queried at a time. When a variable is a reference to a previously mentioned variable name, the variable name must match the previously mentioned variable name, and only the variable name needs to be provided; the entity type is only given when it is first introduced.\"\n      },\n      {\n          \"functionName\": \"count\",\n          \"function_declaration\": \"count(alias)->count_alias\",\n          \"description\": \"Count the number of nodes. The parameter should be a specified set of nodes to count, and it can only be variable names that appear in the get_spo query. The variable name 'count_alias' represents the counting result, which must be of int type, and this variable name can be used for reference in subsequent mentions.\"\n      },\n      {\n          \"functionName\": \"sum\",\n          \"function_declaration\": \"sum(alias, num1, num2, ...)->sum_alias\",\n          \"description\": \"Calculate the sum of data. The parameter should be a specified set to sum, which can be either numbers or variable names mentioned earlier, and its content must be of numeric type. The variable name 'sum_alias' represents the result of the calculation, which must be of numeric type, and this variable name can be used for reference in subsequent mentions.\"      },\n      {\n          \"functionName\": \"sort\",\n          \"function_declaration\": \"sort(set=alias, orderby=o_alias or count_alias or sum_alias, direction=min or max, limit=N)\",\n          \"description\": \"Sort a set of nodes. The 'set' parameter specifies the set of nodes to be sorted and can only be variable names that appear in the get_spo query. The 'orderby' parameter specifies the basis for sorting, which can be the relationship or attribute name of the nodes. If it has been mentioned earlier, an alias should be used. The 'direction' parameter specifies the sorting order, which can only be 'min' (ascending) or 'max' (descending). The 'limit' parameter specifies the limit on the number of output results and must be of int type. The sorted result can be used as the final output.\"      },\n      {\n          \"functionName\": \"compare\",\n          \"function_declaration\": \"compare(set=[alias1, alias2, ...], op=min|max)\",\n          \"description\": \"Compare nodes or numeric values. The 'set' parameter specifies the set of nodes or values to be compared, which can be variable names that appear in the get_spo query or constants. The 'op' parameter specifies the comparison operation: 'min' to find the smallest and 'max' to find the largest.\"\n      },\n      {\n          \"functionName\": \"get\",\n          \"function_decl:aration\": \"get(alias)\",\n          \"description\": \"Return the information represented by a specified alias. This can be an entity, a relationship path, or an attribute value obtained in the get_spo query. It can be used as the final output result.\"\n      }\n    ],\n    \"examples\": [\n        {\n            \"query\": \"Which sports team for which Cristiano Ronaldo played in 2011 was founded last ?\",\n            \"answer\": [ {\"Step\":\"Which Sports Teams Cristiano Ronaldo Played for in 2011 ?\",\n                         \"Action\" : \"get_spo(s=s1:Player[Cristiano Ronaldo],p=p1:PlayedForIn2011Year,o=o1:SportsTeam)\"},\n                        {\"Step\": \"In which year were these teams established ?\",\n                         \"Action\": \"get_spo(s=o1,p=p2:FoundationYear,o=o2:Year)\"},\n                        {\"Step\": \"Which team was founded last ?\",\n                         \"Action\": \"sort(set=o1, orderby=o2, direction=max, limit=1)\"}\n                      ]\n        },\n        {\n            \"query\": \"Who was the first president of the association which published Journal of Psychotherapy Integration?\",\n            \"answer\": [ {\"Step\": \"Which association that publishes the Journal of Psychotherapy Integration ?\",\n                         \"Action\":\"Journal(s=s1:Player[Psychotherapy Integration],p=p1:Publish,o=o1:Association)\"},\n                        {\"Step\": \"Who was the first president of that specific association?\",\n                        \"Action\":\"get_spo(s=o1,p=p2:FirstPresident,o=o2:Person)\"}\n                      ]\n        },\n        {\n            \"query\": \"When did the state where Pocahontas Mounds is located become part of the United States?\",\n            \"answer\": [ {\"Step\": \"Which State Where Pocahontas Mounds is Located ?\",\n                         \"Action\": \"get_spo(s=s1:HistoricalSite[Pocahontas Mounds], p=p1:LocatedIn, o=o1:State)\"},\n                        {\"Step\":\"When did this state become a part of the United States ï¼Ÿ\",\n                         \"Action\": \"get_spo(s=o1, p=p2:YearOfBecamingPartofTheUnitedStates, o=o2:Date)\"}\n                      ]\n        },\n        {\n            \"query\": \"Which of the two tornado outbreaks killed the most people?\",\n            \"answer\": [ {\"Step\": \"Which is the first tornado outbreaks ?\",\n                         \"Action\": \"get_spo(s=s1:Event[Tornado Outbreak], p=p1:TheFirst, o=o1:Event)\"},\n                         {\"Step\": \"Which is the second tornado outbreaks ?\",\n                          \"Action\": \"get_spo(s=s2:Event[Tornado Outbreak], p=p2:TheSecond, o=o2:Event)\"},\n                         { \"Step\": \"How many people died in the first tornado outbreak ?\",\n                           \"Action\": \"get_spo(s=s1, p=p3:KilledPeopleNumber, o=o3:Number)\"},\n                          { \"Step\": \"How many people died in the second tornado outbreak ?\",\n                           \" Action\": \"get_spo(s=s2, p=p4:KilledPeopleNumber, o=o4:Number)\"},\n                           { \"Step\": \"To compare the death toll between two tornado outbreaks to determine which one had more fatalities.\",\n                             \"Action\": \"compare(set=[o3,o4], op=max)\" }\n                      }\n        }\n    ],\n    \"output_format\": \"Only output words in answer, for examples: `Step`, `Action` content\",\n    \"query\": {{ question }}\n    }     \n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest laibhav_resume {\n  functions [LogicFromPlan]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "question_ner.baml": "// Defining a data model.\nclass QNer {\n  name string\n  category string\n}\n\nclass QNers {\n    output QNer[]\n}\n\n// Create a function to extract the resume from a string.\nfunction QuestionNER(schema: string[], input: string) -> QNers {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"Ollama\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    You are an expert in named entity recognition. Please extract entities and that match the schema definition from the input. Please respond in the format of a JSON string. You can refer to the example for extraction.\n    \n    schema to use: \n    \n    {{ schema }}\n\n    example:\n    \n      input: \"Which magazine was started first, Arthur's Magazine or First for Women?\"\n      \n      output:\n      \n      [\n          {\n              \"name\": \"First for Women\",\n              \"category\": \"Works\"\n          },\n          {\n              \"name\": \"Arthur's Magazine\",\n              \"category\": \"Works\"\n          }\n       ]\n\n    \n    input:\n    \n    {{ input }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest laibhav_resume {\n  functions [QuestionNER]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "resp_extractor.baml": "// Defining a data model.\nclass Passage {\n  passage string\n}\n\n// Create a function to extract the resume from a string.\nfunction RespExtractor(supporting_fact: string, instruction: string) -> Passage {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"Ollama\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"  \n    Your job is to act as a professional writer. You will write a good-quality passage that can support the given prediction about the question only based on the information in the provided supporting passages. Now, let's start.\n    \n    Question:\n    \n    {{ instruction }}\n \n    Known information:\n    \n    {{ supporting_fact }}\n \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest laibhav_resume {\n  functions [RespExtractor]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "resp_generator.baml": "// Defining a data model.\nclass Answer {\n  answer string\n}\n\n// Create a function to extract the resume from a string.\nfunction RespGenerator(memory: string, instruction: string) -> Answer {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"Ollama\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Answer the question based on the given reference. Give me the answer and why. \n    \n    The following are given reference:\n    \n    {{ memory }}\n    \n    Question: \n    \n    {{ instruction }}\n \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest laibhav_resume {\n  functions [RespGenerator]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "resp_judge.baml": "// Defining a data model.\nclass Judgement {\n  answer bool\n}\n\n// Create a function to extract the resume from a string.\nfunction RespJudge(memory: string, instruction: string) -> Judgement {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"Ollama\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Judging based solely on the current known information and without allowing for inference, are you able to completely and accurately respond to the question '{{ instruction }}'? \"\n    \n    Known information: \n    \n    {{ memory }}\n\n    If you can, please reply set answer to true; if you cannot and need more information, please set anser to false.\n \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest laibhav_resume {\n  functions [RespJudge]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "resp_reflector.baml": "// Defining a data model.\nclass Reflection {\n  thought_questions string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction RespReflector(memory: string, instruction: string) -> Reflection {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"Ollama\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    You serve as an intelligent assistant, adept at facilitating users through complex, multi-hop reasoning across multiple documents. Please understand the information gap between the currently known information and the target problem. Your task is to generate one thought in the form of question for next retrieval step directly. DON'T generate the whole thoughts at once!\n    \n    Known information: \n    \n    {{ memory }}\n    \n    Target question:\n    \n    {{ instruction }} \n \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest laibhav_resume {\n  functions [RespReflector]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "resp_verifier.baml": "// Defining a data model.\nclass Verifier {\n  answer string\n  answered bool\n}\n\n// Create a function to extract the resume from a string.\nfunction RespVerifier(sub_instruction: string, supporting_fact: string) -> Verifier {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"Ollama\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Judging based solely on the current known information and without allowing for inference, are you able to respond completely and accurately to the question '{{ sub_instruction }}'?\n    If yes, please set the answered boolean to true, and set the answer field with an accurate response to the question '{{ sub_instruction }}', without restating the question; if no, please set answered boolean to false directly.\n \n    Known information: \n    -----\n    {{ supporting_fact }} \n    -----\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest laibhav_resume {\n  functions [RespVerifier]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "solve_question.baml": "// Defining a data model.\nclass Solve {\n  answer string\n}\n\n// Create a function to extract the resume from a string.\nfunction SolveQuestion(question: string, history: string, docs: string, knowledge_graph: string) -> SolveNoSPO {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"Ollama\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Please answer the question below based on the retrieved knowledge graph and relevant documents, and combine historical information for comprehensive analysis.\n    \n    Requirement:\n        1. Answer the question as directly as possible, without including any other information.\n        2. Do not repeat the content of the question.\n        3. Generate answers based on the provided information. If multiple answers are possible, generate all of them.\n        4. If there is no suitable answer, answer 'I don't know'.\n        5. Provide the answer and also provide the reason.\n\n    question to answer:\n\n    {{ question }}\n\n    history:\n    \n    {{ history }}\n\n    knowledge graph to use to answer question:\n\n    {{ knowledge_graph }}\n\n    relevant documents to use to answer question:\n    \n    {{ docs }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest laibhav_resume {\n  functions [SolveQuestion]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "solve_question_without_docs.baml": "// Defining a data model.\nclass SolveNoDocs {\n  answer string\n}\n\n// Create a function to extract the resume from a string.\nfunction SolveQuestionNoDocs(question: string, history: string, knowledge_graph: string) -> SolveNoDocs {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"Ollama\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Please answer the question below based on the retrieved knowledge graph, combine historical information for comprehensive analysis. \n\n    Requirement:\n      1. Answer the question as directly as possible, without including any other information.\n      2. Do not repeat the content of the question.\n      3. Generate answers based on the provided information. If multiple answers are possible, generate all of them.\n      4. If there is no suitable answer, answer 'I don't know'.\n      5. Provide the answer and also provide the reason.\n\n    the question to answer:\n\n    {{ question }}\n\n    history:\n\n    {{ history }}\n\n    knowledge graph to use to answer question:\n    \n    {{ knowledge_graph }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest laibhav_resume {\n  functions [SolveQuestionNoDocs]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "solve_question_without_spo.baml": "// Defining a data model.\nclass SolveNoSPO {\n  answer string\n}\n\n// Create a function to extract the resume from a string.\nfunction SolveQuestionNoSPO(question: string, history: string, docs: string) -> SolveNoSPO {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"Ollama\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Please answer the question below based on the retrieved relevant documents, and combine historical information for comprehensive analysis.\n    \n    Requirements in ansering question:\n        1. Answer the question as directly as possible, without including any other information.\n        2. Do not repeat the content of the question.\n        3. Generate answers based on the provided information. If multiple answers are possible, generate all of them.\n        4. If there is no suitable answer, answer 'I don't know'.\n        5. Provide the answer and also provide the reason.\n\n    the question to answer:\n\n    {{ question }}\n  \n    history:\n    \n    {{ history }}\n\n    relevant documents to use to answer the question:\n\n    {{ docs }}\n\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest laibhav_resume {\n  functions [SolveQuestionNoSPO]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "spo_retrieval.baml": "// Defining a data model.\nclass SPOs {\n  spos string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction SPORetrieval(question: string, mention: string, candis: string[])-> SPOs {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"Ollama\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    You are a language expert. Your task is to select the correct SPO (Subject Predicate Object) text from the given SPO candidates according to the following rules to answer the given question. Ensure that the selected SPO matches the SPO mention or appropriately answers the question. Provide a JSON list of the selected SPO candidates that best answer the question based on the spo mention.\n\n    requirements: \n\n    1. The output must be selected from the SPO candidates and remain consistent with their content, presented in a list format.\n    2. If there is no suitable answer in the SPO candidates, output an empty list. Ensure that the output is highly relevant to the question or SPO mention.\n    3. If there are multiple correct answers in the SPO candidates, output all matching SPOs in a JSON list format.\n  \n    example1: \n\n        question: Is the Woman's Viewpoint a British publication?\n        spo_mention: Publication[Woman's Viewpoint] Nationality Country\n        spo_candidates: \n        \n        [\n            \"the woman s viewpoint ranFrom 1923 to 1927\",\n            \"the woman s viewpoint publishedBy florence m sterling\",\n            \"the woman s viewpoint foundedIn 1923\",\n            \"the woman s viewpoint foundedIn texas\",\n            \"the woman s viewpoint was a woman s magazine\",\n            \"rolandos liatsos starredIn woman in mind\",\n            \"rolandos liatsos starredIn woman in mind\"\n        ]\n\n        analysis: The question seeks the nationality of the publication \"Woman's Viewpoint.\" Among the SPO candidates, \"the woman s viewpoint foundedIn texas\" indicates the location of its founding, which relates to its nationality.\n\n        output:\n\n        { spos: [\n              \"the woman s viewpoint foundedIn texas\"\n          ]\n        }\n  \n    example2:\n\n      question: Who is the German musician whose hand the manuscript for Flute Sonata in C major, BWV 1033 is in?\n      spo_mention: Entity[Flute Sonata in C major, BWV 1033] InHandOf Entity\n      spo_candidates: \n      \n        [\n              \"flute sonata in c major bwv 1033 isAttributedTo johann sebastian bach\",\n              \"flute sonata in c major bwv 1033 isFor flute or recorder and basso continuo\",\n              \"flute sonata in c major bwv 1033 is a sonata in 4 movements\"\n        ]\n\n      analysis: The question aims to identify who holds the manuscript of \"Flute Sonata in C major, BWV 1033.\" The SPO candidate \"flute sonata in c major bwv 1033 isAttributedTo johann sebastian bach\" implies ownership, indicating that Johann Sebastian Bach holds the manuscript.\n      output:\n\n      { spo: [\n            \"flute sonata in c major bwv 1033 isAttributedTo johann sebastian bach\"\n        ]\n      }\n\n    question: \n    \n    {{ question }}\n\n    spo_mention: \n    \n    {{ mention }}\n\n    spo_candidates: \n    \n    {{ candis }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest laibhav_resume {\n  functions [SPORetrieval]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
}

def get_baml_files():
    return file_map